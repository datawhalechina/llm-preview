{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3276edeb",
   "metadata": {},
   "source": [
    "# Chapter-2 å¤§æ¨¡å‹ä½¿ç”¨\n",
    "\n",
    "æœ¬èŠ‚å­¦ä¹ å¤§çº²\n",
    "\n",
    "- æŒæ¡é€šè¿‡ API è°ƒç”¨äº‘ç«¯å¤§æ¨¡å‹çš„åŸºæœ¬æ–¹æ³•ã€‚\n",
    "- æŒæ¡ä½¿ç”¨ transformers åº“åœ¨æœ¬åœ°è¿è¡Œå¤§æ¨¡å‹çš„åŸºæœ¬æµç¨‹ã€‚\n",
    "- äº†è§£ vLLM ç­‰é«˜æ€§èƒ½æ¨ç†æ¡†æ¶çš„ç”¨é€”å’ŒåŸºæœ¬ä½¿ç”¨æ–¹æ³•ã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a89e5ec",
   "metadata": {},
   "source": [
    "ä¸Šä¸€èŠ‚è¯¾æˆ‘ä»¬ä¸»è¦è§£äº†ä»€ä¹ˆæ˜¯å¤§æ¨¡å‹ï¼Œä»¥åŠåœ¨å“ªé‡Œå¯ä»¥æ‰¾åˆ°å®ƒä»¬ï¼Œæ¯”å¦‚ Hugging Face Hub ä»¥åŠ ModelScopeã€‚æœ¬èŠ‚çš„ä¸»è¦ç›®æ ‡æ˜¯â€œåŠ¨æ‰‹â€ï¼Œä»ç†è®ºèµ°å‘å®è·µï¼ŒçœŸæ­£åœ°æŠŠå¤§æ¨¡å‹ç”¨èµ·æ¥ã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "984e262d",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "ä½¿ç”¨å¤§æ¨¡å‹ä¸»è¦æœ‰ä¸¤ç§è·¯å¾„ï¼š\n",
    "\n",
    "1. äº‘ç«¯APIè°ƒç”¨ï¼šåƒä½¿ç”¨ä¸€ä¸ªåœ¨çº¿æœåŠ¡ä¸€æ ·ï¼Œæˆ‘ä»¬å‘é€è¯·æ±‚ï¼Œè¿œç«¯çš„æœåŠ¡å™¨ï¼ˆäº‘ï¼‰è¿”å›ç»“æœã€‚è¿™ç§æ–¹å¼ç®€å•ã€æ–¹ä¾¿ï¼Œå¯¹æˆ‘ä»¬è‡ªå·±çš„ç”µè„‘é…ç½®è¦æ±‚ä¸é«˜ã€‚\n",
    "\n",
    "2. æœ¬åœ°åŒ–éƒ¨ç½²ï¼šæŠŠå¤§æ¨¡å‹ä¸‹è½½åˆ°æˆ‘ä»¬è‡ªå·±çš„ç”µè„‘ï¼ˆæœåŠ¡å™¨ï¼‰ä¸Šå¹¶è¿è¡Œèµ·æ¥ã€‚è¿™ç§æ–¹å¼ç»™äº†æˆ‘ä»¬æœ€å¤§çš„æ§åˆ¶æƒå’Œéšç§ä¿éšœï¼Œä½†å¯¹ç¡¬ä»¶ï¼ˆå°¤å…¶æ˜¯æ˜¾å¡ï¼‰æœ‰ä¸€å®šè¦æ±‚ã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "955ff506",
   "metadata": {},
   "source": [
    "## 2.1 äº‘ç«¯å¤§æ¨¡å‹è°ƒç”¨ï¼šæœ€ä¾¿æ·çš„è·¯å¾„\n",
    "\n",
    "è¿™æ˜¯ä½¿ç”¨å¤§æ¨¡å‹æœ€ç›´æ¥çš„æ–¹å¼ã€‚æˆ‘ä»¬ä¸éœ€è¦å…³å¿ƒç¡¬ä»¶é…ç½®ã€æ¨¡å‹åŠ è½½ç­‰å¤æ‚é—®é¢˜ï¼Œåªéœ€è¦è·å¾—ä¸€ä¸ªè®¸å¯ï¼ˆAPI Keyï¼‰ï¼Œç„¶åæŒ‰ç…§æœåŠ¡å•†æä¾›çš„åœ°å€å’Œæ ¼å¼å‘é€è¯·æ±‚å³å¯ã€‚\n",
    "\n",
    "æˆ‘ä»¬å°†ä»¥[ç¡…åŸºæµåŠ¨ï¼ˆSiliconFlowï¼‰](https://cloud.siliconflow.cn/i/ybUFvmqK)å¹³å°ä¸ºä¾‹ï¼Œå®ƒæä¾›äº†ä¸ OpenAI API å®Œå…¨å…¼å®¹çš„æ¥å£ï¼Œæˆ‘ä»¬å¯ä»¥ç›´æ¥ä½¿ç”¨ OpenAI ä¸­çš„ Python åº“è°ƒç”¨æ¨¡å‹ã€‚\n",
    "\n",
    "> æ³¨ï¼šè™½ç„¶æˆ‘ä»¬ä½¿ç”¨ `openai` åº“çš„æ–¹å¼æ¥è°ƒç”¨äº‘ç«¯å¤§æ¨¡å‹ï¼Œä½†å…¶æœ¬è´¨æ˜¯å‘è¿œç¨‹æœåŠ¡å™¨å‘é€è¯·æ±‚ï¼Œæ‰€ä»¥åŒå­¦ä»¬ä¹Ÿå¯ä»¥ä½¿ç”¨ python ä¸­çš„ `requests` åº“ï¼Œä»¥å‘é€ç½‘ç»œè¯·æ±‚çš„æ–¹å¼ä¹Ÿå¯ä»¥å®ç°äº‘ç«¯å¤§æ¨¡å‹çš„è°ƒç”¨ã€‚å­¦æœ‰ä½™åŠ›çš„åŒå­¦å¯ä»¥è‡ªè¡Œæ¢ç´¢ä¸€ä¸‹ï½"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62bbf606",
   "metadata": {},
   "source": [
    "### Step 1ï¼šè·å– [ç¡…åŸºæµåŠ¨](https://cloud.siliconflow.cn/i/ybUFvmqK) API Key\n",
    "\n",
    "è®¿é—®ç¡…åŸºæµåŠ¨å¹³å°å®˜ç½‘ï¼Œæ³¨å†Œè´¦å·ï¼Œåœ¨ä¸ªäººä¸­å¿ƒæ‰¾åˆ°ä½ çš„ API Keyï¼ˆå¯†é’¥ï¼‰ å’Œ API Base URLï¼ˆæ¥å£åœ°å€ï¼‰ã€‚è¯·åŠ¡å¿…ä¿ç®¡å¥½ä½ çš„ API Keyï¼Œä¸è¦æ³„éœ²ã€‚\n",
    "\n",
    "å¦‚ä¸‹å›¾æ‰€ç¤ºï¼Œåˆ›å»ºå¥½è´¦æˆ·ä¹‹åï¼Œå³å¯è·å–åˆ° API Key å’Œ API Base URLã€‚\n",
    "\n",
    "![](./images/2-1%20ç¡…åŸºæµåŠ¨api-keyåˆ›å»º.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "330cee35",
   "metadata": {},
   "source": [
    "### Step 2: è°ƒç”¨å¤§æ¨¡å‹\n",
    "\n",
    "æ¥ä¸‹æ¥ï¼Œæˆ‘ä»¬å®‰è£…`openai`åº“ï¼Œç”¨äºè°ƒç”¨å¤§æ¨¡å‹ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b882b38",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q openai"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80d90054",
   "metadata": {},
   "source": [
    "ä½¿ç”¨ä»¥ä¸‹ä»£ç è°ƒç”¨äº‘ç«¯APIå¤§æ¨¡å‹ï¼š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7576f273",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "ä½ å¥½ï¼ğŸ˜Š æœ‰ä»€ä¹ˆæˆ‘å¯ä»¥å¸®ä½ çš„å—ï¼Ÿæ— è®ºæ˜¯è§£ç­”é—®é¢˜ã€æä¾›å»ºè®®ï¼Œè¿˜æ˜¯åªæ˜¯é—²èŠï¼Œæˆ‘éƒ½å¾ˆä¹æ„å’Œä½ äº¤æµï¼\n"
     ]
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI(api_key=\"your api key\", \n",
    "                base_url=\"https://api.siliconflow.cn/v1\")\n",
    "response = client.chat.completions.create(\n",
    "    model=\"Qwen/Qwen3-8B\",\n",
    "    messages=[\n",
    "        {'role': 'user', 'content': \"ä½ å¥½ï¼\"}\n",
    "    ],\n",
    "    max_tokens=1024,\n",
    "    temperature=0.7,\n",
    "    stream=False\n",
    ")\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef8e29ef",
   "metadata": {},
   "source": [
    "- temperatureï¼ˆæ¸©åº¦ï¼‰\n",
    "  - æ§åˆ¶è¾“å‡ºçš„éšæœºæ€§/åˆ›é€ æ€§\n",
    "  - å–å€¼èŒƒå›´ 0-1ï¼Œå€¼è¶Šå¤§éšæœºæ€§è¶Šå¼º\n",
    "  - 0è¡¨ç¤ºå§‹ç»ˆé€‰æ‹©æœ€å¯èƒ½çš„è¯\n",
    "\n",
    "- max_tokensï¼ˆæœ€å¤§ä»¤ç‰Œæ•°ï¼‰\n",
    "  - é™åˆ¶æ¨¡å‹å›å¤çš„æœ€å¤§é•¿åº¦\n",
    "  - 1 token çº¦ç­‰äº 4 ä¸ªå­—ç¬¦\n",
    "  - éœ€è¦æ ¹æ®å…·ä½“ä»»åŠ¡è®¾ç½®åˆé€‚çš„å€¼\n",
    "\n",
    "- stream\n",
    "  - æ˜¯å¦æµå¼è¾“å‡º\n",
    "  - å–å€¼èŒƒå›´ True/False\n",
    "  - True è¡¨ç¤ºæµå¼è¾“å‡ºï¼Œæ¨¡å‹ä¼šè¾¹ç”Ÿæˆè¾¹è¾“å‡º\n",
    "  - False è¡¨ç¤ºä¸€æ¬¡æ€§è¾“å‡ºå…¨éƒ¨ç»“æœ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd34b964",
   "metadata": {},
   "source": [
    "æˆ‘ä»¬æŠŠä»¥ä¸Šä»£ç å°è£…ä¸ºä¸€ä¸ªå‡½æ•°ï¼Œå¯ä»¥æ¥å—ç”¨æˆ·è¾“å…¥ï¼Œå†å²å¯¹è¯å’Œç³»ç»Ÿæç¤ºè¯ï¼Œæœ€ç»ˆè¿”å›æ¨¡å‹ç”Ÿæˆçš„æ–‡æœ¬ï¼š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf562f56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "ä½ å¥½å‘€ï¼ä»Šå¤©è¿‡å¾—æ€ä¹ˆæ ·å‘€ï¼Ÿæœ‰ä»€ä¹ˆæˆ‘å¯ä»¥å¸®ä½ çš„å—ï¼ŸğŸ˜Š\n"
     ]
    }
   ],
   "source": [
    "def chat_with_model(user_input: str, history: list = None, temperature: float = 0.7, system_prompt: str = None) -> str:\n",
    "    \"\"\"\n",
    "    ä¸å¤§æ¨¡å‹è¿›è¡Œå¯¹è¯çš„å‡½æ•°\n",
    "    \n",
    "    Args:\n",
    "        user_input: ç”¨æˆ·è¾“å…¥çš„æ–‡æœ¬\n",
    "        history: å†å²å¯¹è¯è®°å½•åˆ—è¡¨\n",
    "        temperature: æ¸©åº¦å‚æ•°ï¼Œæ§åˆ¶è¾“å‡ºçš„éšæœºæ€§\n",
    "        system_prompt: ç³»ç»Ÿæç¤ºè¯\n",
    "    \n",
    "    Returns:\n",
    "        str: æ¨¡å‹è¿”å›çš„æ–‡æœ¬\n",
    "    \"\"\"\n",
    "\n",
    "    # åˆå§‹åŒ– OpenAI å®¢æˆ·ç«¯\n",
    "    client = OpenAI(api_key=\"your api key\", \n",
    "                base_url=\"https://api.siliconflow.cn/v1\")\n",
    "\n",
    "    # åˆå§‹åŒ–å†å²è®°å½•\n",
    "    if history is None:\n",
    "        history = []\n",
    "        \n",
    "    # æ„å»ºæ¶ˆæ¯åˆ—è¡¨\n",
    "    messages = []\n",
    "    \n",
    "    # æ·»åŠ ç³»ç»Ÿæç¤ºè¯(å¦‚æœæœ‰)\n",
    "    if system_prompt:\n",
    "        messages.append({\"role\": \"system\", \"content\": system_prompt})\n",
    "    \n",
    "    # æ·»åŠ å†å²å¯¹è¯\n",
    "    for msg in history:\n",
    "        messages.append(msg)\n",
    "    \n",
    "    # æ·»åŠ å½“å‰ç”¨æˆ·è¾“å…¥\n",
    "    messages.append({\"role\": \"user\", \"content\": user_input})\n",
    "    \n",
    "    # è°ƒç”¨APIè·å–å“åº”\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"Qwen/Qwen3-8B\",\n",
    "        messages=messages,\n",
    "        temperature=temperature\n",
    "    )\n",
    "    \n",
    "    # è¿”å›æ¨¡å‹å›å¤çš„æ–‡æœ¬\n",
    "    return response.choices[0].message.content\n",
    "\n",
    "\n",
    "print(chat_with_model(\"ä½ å¥½å“‡\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dcfee3a",
   "metadata": {},
   "source": [
    "## 2.2 æœ¬åœ°éƒ¨ç½²ä¸è°ƒç”¨ï¼šæ›´å¼ºçš„æŒæ§åŠ›\n",
    "\n",
    "è°ƒç”¨äº‘ç«¯çš„å¤§æ¨¡å‹å›ºç„¶å¾ˆæ–¹ä¾¿ï¼Œä½†å®ƒå­˜åœ¨ä¸€äº›é—®é¢˜ã€‚æ¯”å¦‚ï¼šæ•°æ®çš„éšç§æ€§ã€‚å½“åŒå­¦ä»¬å¯¹å¤–å‘é€è¯·æ±‚çš„æ—¶å€™ï¼Œæœ¬åœ°æ•°æ®å·²ç»æ³„æ¼ç»™å¤–ç•Œäº†ã€‚æ‰€ä»¥å½“æˆ‘ä»¬ç”¨ä¸€äº›ç‰¹æ®Šéœ€æ±‚æ—¶ï¼Œå¾€å¾€ä¼šé‡‡ç”¨æœ¬åœ°éƒ¨ç½²å¤§æ¨¡å‹ç„¶åè¿›è¡Œè°ƒç”¨ã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12033d60",
   "metadata": {},
   "source": [
    "### 2.2.1 ä½¿ç”¨ `transformers` éƒ¨ç½²å¤§æ¨¡å‹\n",
    "\n",
    "`transformers` æ˜¯ Hugging Face å…¬å¸å¼€å‘çš„ä¸€ä¸ªåº“ï¼Œæ˜¯å½“ä¸‹è¿›è¡ŒNLPï¼ˆè‡ªç„¶è¯­è¨€å¤„ç†ï¼‰å’Œæ¨¡å‹ç ”ç©¶å¼€å‘çš„æœ€æµè¡Œçš„ä¸€ä¸ªpythonåº“ã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27c98029",
   "metadata": {},
   "source": [
    "> æ³¨ï¼šä¸çŸ¥é“å¤§å®¶æœ‰æ²¡æœ‰å®Œæˆä¸Šä¸€èŠ‚å¸ƒç½®çš„è¯¾åä½œä¸šï¼Ÿï¼ˆä¸‹è½½ Qwen3-4B æ¨¡å‹ï¼‰\n",
    "\n",
    "æ²¡æœ‰ä¸‹è½½å¥½çš„åŒå­¦å¯ä»¥è¿è¡Œä»¥ä¸‹ä»£ç ï¼Œåœ¨ ModelScope å¹³å°ä¸‹è½½æ¨¡å‹ã€‚æ¨¡å‹å¤§å°åœ¨ 8G å·¦å³ã€‚\n",
    "\n",
    "> æ³¨ï¼šå¦‚æœæ¨¡å‹ä¸‹è½½çš„ç¯å¢ƒä¹Ÿæ²¡æœ‰é…ç½®å®Œå…¨ï¼Œå¯ä»¥å†é˜…è¯»ä»¥ä¸‹ç¬¬ä¸€èŠ‚çš„å†…å®¹ï¼Œè¿›è¡Œç¯å¢ƒé…ç½®ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e50069c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from modelscope import snapshot_download\n",
    "\n",
    "model_dir = snapshot_download('Qwen/Qwen3-4B', cache_dir='/home/mw/model', revision='master')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e786dc6",
   "metadata": {},
   "source": [
    "æ¨¡å‹ä¸‹è½½å®Œæˆä¹‹åï¼Œå°±å¯ä»¥ä½¿ç”¨ `transformers` åº“åŠ è½½æ¨¡å‹äº†ï¼Œé¦–å…ˆæˆ‘ä»¬éœ€è¦å®‰è£…ä¸€äº›åŠ è½½æ¨¡å‹çš„ç¯å¢ƒé…ç½®ï¼Œå¦‚æœåŒå­¦ä»¬åœ¨ç¬¬ä¸€èŠ‚å®‰è£…è¿‡äº†ï¼Œé‚£å°±ä¸éœ€è¦å†å®‰è£…äº†ã€‚\n",
    "\n",
    "```python\n",
    "!pip install -q modelscope transformers accelerate\n",
    "```\n",
    "\n",
    "> æ³¨ï¼šæ²¡æœ‰é…ç½®ç¯å¢ƒçš„åŒå­¦ï¼Œè¯·å¤åˆ¶ä»¥ä¸Šä»£ç åˆ°è‡ªå·±çš„ç¯å¢ƒä¸­è¿è¡Œå®‰è£…ã€‚\n",
    "\n",
    "é…ç½®å¥½ç¯å¢ƒçš„åŒå­¦ï¼Œå¯ä»¥ä½¿ç”¨ä»¥ä¸‹ä»£ç è¿è¡ŒåŠ è½½æ¨¡å‹ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c038fb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "# è®¾ç½®æ¨¡å‹æœ¬åœ°è·¯å¾„\n",
    "model_name = \"/home/mw/model/Qwen/Qwen3-4B\"\n",
    "\n",
    "# åŠ è½½åˆ†è¯å™¨å’Œæ¨¡å‹\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    torch_dtype=\"auto\",  # è‡ªåŠ¨é€‰æ‹©åˆé€‚çš„æ•°æ®ç±»å‹\n",
    "    device_map=\"cuda:0\",    # è‡ªåŠ¨é€‰æ‹©å¯ç”¨è®¾å¤‡(CPU/GPU)\n",
    "    trust_remote_code=True\n",
    ")\n",
    "\n",
    "# å‡†å¤‡æ¨¡å‹è¾“å…¥\n",
    "prompt = \"ä½ å¥½ï¼Œè¯·ä»‹ç»ä¸€ä¸‹è‡ªå·±\"\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": prompt}\n",
    "]\n",
    "text = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    tokenize=False,\n",
    "    add_generation_prompt=True,\n",
    "    enable_thinking=True # é€‰æ‹©æ˜¯å¦æ‰“å¼€æ·±åº¦æ¨ç†æ¨¡å¼\n",
    ")\n",
    "# å°†è¾“å…¥æ–‡æœ¬è½¬æ¢ä¸ºæ¨¡å‹å¯å¤„ç†çš„å¼ é‡æ ¼å¼\n",
    "model_inputs = tokenizer([text], return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "# ç”Ÿæˆæ–‡æœ¬\n",
    "generated_ids = model.generate(\n",
    "    **model_inputs,\n",
    "    max_new_tokens=32768  # è®¾ç½®æœ€å¤§ç”Ÿæˆtokenæ•°é‡\n",
    ")\n",
    "# æå–æ–°ç”Ÿæˆçš„token ID\n",
    "output_ids = generated_ids[0][len(model_inputs.input_ids[0]):].tolist() \n",
    "\n",
    "# è§£ææ€è€ƒå†…å®¹\n",
    "try:\n",
    "    # rindex finding 151668 (</think>)\n",
    "    # æŸ¥æ‰¾ç»“æŸæ ‡è®°\"</think>\"çš„ä½ç½®\n",
    "    index = len(output_ids) - output_ids[::-1].index(151668)\n",
    "except ValueError:\n",
    "    index = 0\n",
    "\n",
    "# è§£ç æ€è€ƒå†…å®¹å’Œæœ€ç»ˆå›ç­”\n",
    "thinking_content = tokenizer.decode(output_ids[:index], skip_special_tokens=True).strip(\"\\n\")\n",
    "content = tokenizer.decode(output_ids[index:], skip_special_tokens=True).strip(\"\\n\")\n",
    "\n",
    "# æ‰“å°ç»“æœ\n",
    "print(\"thinking content:\", thinking_content)\n",
    "print(\"content:\", content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70decac1",
   "metadata": {},
   "source": [
    "åŒæ ·æˆ‘ä»¬ä¹Ÿå¯ä»¥å°†å…¶å°è£…ä¸ºä¸€ä¸ªå‡½æ•°ï¼Œå¦‚ä¸‹ä»£ç æ‰€ç¤ºï¼š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22e06cf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chat_with_local_model(model_path: str, user_input: str, history: list = None, temperature: float = 0.7, max_new_tokens: int = 1024, enable_thinking: bool = True) -> dict:\n",
    "    \"\"\"\n",
    "    ä½¿ç”¨æœ¬åœ°å¤§æ¨¡å‹è¿›è¡Œå¯¹è¯çš„å‡½æ•°\n",
    "    \n",
    "    Args:\n",
    "        model_path: æ¨¡å‹æœ¬åœ°è·¯å¾„\n",
    "        user_input: ç”¨æˆ·è¾“å…¥çš„æ–‡æœ¬\n",
    "        history: å†å²å¯¹è¯è®°å½•åˆ—è¡¨ï¼Œæ ¼å¼ä¸º [{\"role\": \"user\", \"content\": \"...\"}, {\"role\": \"assistant\", \"content\": \"...\"}]\n",
    "        temperature: æ¸©åº¦å‚æ•°ï¼Œæ§åˆ¶è¾“å‡ºçš„éšæœºæ€§ (0.0-1.0)\n",
    "        max_new_tokens: æœ€å¤§ç”Ÿæˆtokenæ•°é‡\n",
    "        enable_thinking: æ˜¯å¦å¯ç”¨æ·±åº¦æ¨ç†æ¨¡å¼\n",
    "    \n",
    "    Returns:\n",
    "        dict: åŒ…å«æ€è€ƒå†…å®¹å’Œæœ€ç»ˆå›ç­”çš„å­—å…¸ {\"thinking_content\": str, \"content\": str}\n",
    "    \"\"\"\n",
    "    \n",
    "    # åŠ è½½åˆ†è¯å™¨å’Œæ¨¡å‹\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True)\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_path,\n",
    "        torch_dtype=\"auto\",  # è‡ªåŠ¨é€‰æ‹©åˆé€‚çš„æ•°æ®ç±»å‹\n",
    "        device_map=\"cuda:0\",    # è‡ªåŠ¨é€‰æ‹©å¯ç”¨è®¾å¤‡(CPU/GPU)\n",
    "        trust_remote_code=True\n",
    "    )\n",
    "    \n",
    "    # åˆå§‹åŒ–å†å²è®°å½•\n",
    "    if history is None:\n",
    "        history = []\n",
    "    \n",
    "    # æ„å»ºæ¶ˆæ¯åˆ—è¡¨\n",
    "    messages = []\n",
    "    \n",
    "    # æ·»åŠ å†å²å¯¹è¯\n",
    "    for msg in history:\n",
    "        messages.append(msg)\n",
    "    \n",
    "    # æ·»åŠ å½“å‰ç”¨æˆ·è¾“å…¥\n",
    "    messages.append({\"role\": \"user\", \"content\": user_input})\n",
    "    \n",
    "    # åº”ç”¨èŠå¤©æ¨¡æ¿\n",
    "    text = tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        tokenize=False,\n",
    "        add_generation_prompt=True,\n",
    "        enable_thinking=enable_thinking  # é€‰æ‹©æ˜¯å¦æ‰“å¼€æ·±åº¦æ¨ç†æ¨¡å¼\n",
    "    )\n",
    "    \n",
    "    # å°†è¾“å…¥æ–‡æœ¬è½¬æ¢ä¸ºæ¨¡å‹å¯å¤„ç†çš„å¼ é‡æ ¼å¼\n",
    "    model_inputs = tokenizer([text], return_tensors=\"pt\").to(model.device)\n",
    "    \n",
    "    # ç”Ÿæˆæ–‡æœ¬\n",
    "    generated_ids = model.generate(\n",
    "        **model_inputs,\n",
    "        max_new_tokens=max_new_tokens,  # è®¾ç½®æœ€å¤§ç”Ÿæˆtokenæ•°é‡\n",
    "        temperature=temperature,\n",
    "        do_sample=True if temperature > 0 else False  # å½“temperature > 0æ—¶å¯ç”¨é‡‡æ ·\n",
    "    )\n",
    "    \n",
    "    # æå–æ–°ç”Ÿæˆçš„token ID\n",
    "    output_ids = generated_ids[0][len(model_inputs.input_ids[0]):].tolist()\n",
    "    \n",
    "    # è§£ææ€è€ƒå†…å®¹\n",
    "    try:\n",
    "        # rindex finding 151668 (</think>)\n",
    "        # æŸ¥æ‰¾ç»“æŸæ ‡è®°\"</think>\"çš„ä½ç½®\n",
    "        index = len(output_ids) - output_ids[::-1].index(151668)\n",
    "    except ValueError:\n",
    "        index = 0\n",
    "    \n",
    "    # è§£ç æ€è€ƒå†…å®¹å’Œæœ€ç»ˆå›ç­”\n",
    "    thinking_content = tokenizer.decode(output_ids[:index], skip_special_tokens=True).strip(\"\\n\")\n",
    "    content = tokenizer.decode(output_ids[index:], skip_special_tokens=True).strip(\"\\n\")\n",
    "    \n",
    "    return {\n",
    "        \"thinking_content\": thinking_content,\n",
    "        \"content\": content\n",
    "    }\n",
    "\n",
    "# ä½¿ç”¨ç¤ºä¾‹\n",
    "if __name__ == \"__main__\":\n",
    "    # è®¾ç½®æ¨¡å‹è·¯å¾„\n",
    "    model_path = \"/home/mw/model/Qwen/Qwen3-4B\"\n",
    "    \n",
    "    # å•è½®å¯¹è¯ç¤ºä¾‹\n",
    "    result = chat_with_local_model(\n",
    "        model_path=model_path,\n",
    "        user_input=\"ä½ å¥½ï¼Œè¯·ä»‹ç»ä¸€ä¸‹è‡ªå·±\",\n",
    "        temperature=0.7\n",
    "    )\n",
    "    \n",
    "    print(\"thinking content:\", result[\"thinking_content\"])\n",
    "    print(\"content:\", result[\"content\"])\n",
    "    \n",
    "    # å¤šè½®å¯¹è¯ç¤ºä¾‹\n",
    "    history = [\n",
    "        {\"role\": \"user\", \"content\": \"ä½ å¥½\"},\n",
    "        {\"role\": \"assistant\", \"content\": \"ä½ å¥½ï¼æˆ‘æ˜¯Qwenï¼Œä¸€ä¸ªAIåŠ©æ‰‹ã€‚\"}\n",
    "    ]\n",
    "    \n",
    "    result = chat_with_local_model(\n",
    "        model_path=model_path,\n",
    "        user_input=\"ä½ èƒ½å¸®æˆ‘å†™ä¸€é¦–è¯—å—ï¼Ÿ\",\n",
    "        history=history,\n",
    "        temperature=0.8\n",
    "    )\n",
    "    \n",
    "    print(\"\\nå¤šè½®å¯¹è¯ç»“æœ:\")\n",
    "    print(\"thinking content:\", result[\"thinking_content\"])\n",
    "    print(\"content:\", result[\"content\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "494cb8d3",
   "metadata": {},
   "source": [
    "è™½ç„¶ä½¿ç”¨ transformers åœ¨æœ¬åœ°éƒ¨ç½²æ¨¡å‹èƒ½è®©æˆ‘ä»¬è·å¾—å®Œæ•´çš„æ§åˆ¶æƒé™ï¼Œä½†è¿™ç§æ–¹å¼å­˜åœ¨ä¸€å®šçš„æ€§èƒ½ç“¶é¢ˆï¼Œå°¤å…¶åœ¨é¦–æ¬¡æ¨ç†æ—¶è¡¨ç°æ˜æ˜¾ã€‚è¿™ç§æœ¬åœ°éƒ¨ç½²æ–¹å¼æ›´é€‚åˆè¿›è¡Œç®€å•æ¨¡å‹åŠ è½½æµ‹è¯•æˆ–ç®—æ³•ç ”ç©¶å·¥ä½œï¼Œä½†è‹¥è¦å°†æ¨¡å‹æ‰“é€ æˆä¸€ä¸ªæ”¯æŒé«˜å¹¶å‘è°ƒç”¨çš„æœåŠ¡ï¼Œå…¶è®¡ç®—æ•ˆç‡åˆ™æ˜¾å¾—æ‰è¥Ÿè§è‚˜ã€‚è¿™ä¾¿æ˜¯ transformers æœ¬åœ°éƒ¨ç½²æ–¹æ¡ˆåœ¨æ€§èƒ½æ–¹é¢çš„ä¸»è¦å±€é™ã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e7064dc",
   "metadata": {},
   "source": [
    "### 2.2.2 è¿›é˜¶ï¼šä½¿ç”¨ vLLM è¿›è¡Œé«˜æ€§èƒ½éƒ¨ç½²"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b450f444",
   "metadata": {},
   "source": [
    "ä¸ºäº†è§£å†³ä¸Šè¿°æ•ˆç‡é—®é¢˜ï¼Œæˆ‘ä»¬å¯ä»¥ä½¿ç”¨åƒ vLLM è¿™æ ·çš„é«˜æ€§èƒ½æ¨ç†æ¡†æ¶ã€‚vLLM é€šè¿‡ `PagedAttention` ç­‰å…ˆè¿›çš„å†…å­˜ç®¡ç†æŠ€æœ¯ï¼Œæå¤§åœ°æå‡äº†æ¨¡å‹çš„ååé‡ï¼ˆæ¯ç§’èƒ½å¤„ç†çš„è¯·æ±‚æ•°ï¼‰å’Œå“åº”é€Ÿåº¦ï¼Œç‰¹åˆ«æ˜¯åœ¨å¹¶å‘è¯·æ±‚çš„åœºæ™¯ä¸‹ã€‚\n",
    "\n",
    "é¦–å…ˆç¬¬ä¸€æ­¥ï¼Œå®‰è£… vllmï¼Œç›´æ¥å®‰è£…å³å¯ï½\n",
    "\n",
    "```python\n",
    "!pip install vllm\n",
    "```\n",
    "\n",
    "> æ³¨æ„ï¼šå®‰è£…æ—¶é—´è¾ƒé•¿ï¼Œè¯·è€å¿ƒç­‰å¾…ã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "840050fc",
   "metadata": {},
   "source": [
    "ä½¿ç”¨ä»¥ä¸‹å‘½ä»¤åœ¨å‘½ä»¤è¡Œç»ˆç«¯å¯åŠ¨ vllm æœåŠ¡ï¼š\n",
    "\n",
    "```bash\n",
    "vllm serve /home/mw/model/Qwen/Qwen3-4B \\    \n",
    "  --served-model-name Qwen3-4B \\    \n",
    "  --max_model_len 1024 \\    \n",
    "  --enable-reasoning \\    \n",
    "  --reasoning-parser deepseek_r1\n",
    "```\n",
    "\n",
    "ä»¥ä¸Šå‘½ä»¤ä¸­å„å‚æ•°çš„å«ä¹‰å¦‚ä¸‹ï¼š\n",
    "\n",
    "- `/home/mw/model/Qwen/Qwen3-4B`: æ¨¡å‹è·¯å¾„ï¼ŒæŒ‡å‘æœ¬åœ°å­˜å‚¨çš„ Qwen 3.0 6B æ¨¡å‹æ–‡ä»¶\n",
    "- `--served-model-name Qwen3-4B`: æœåŠ¡å¯åŠ¨åä½¿ç”¨çš„æ¨¡å‹åç§°\n",
    "- `--max_model_len 1024`: è®¾ç½®æ¨¡å‹æœ€å¤§å¤„ç†çš„åºåˆ—é•¿åº¦ä¸º 1024 ä¸ª token\n",
    "- `--enable-reasoning`: å¯ç”¨æ¨ç†åŠŸèƒ½\n",
    "- `--reasoning-parser deepseek_r1`: ä½¿ç”¨ deepseek_r1 ä½œä¸ºæ¨ç†è§£æå™¨\n",
    "\n",
    "è¿™äº›å‚æ•°é…ç½®äº† vllm æœåŠ¡çš„åŸºæœ¬è¿è¡Œå‚æ•°ï¼ŒåŒ…æ‹¬æ¨¡å‹ä½ç½®ã€æœåŠ¡åç§°ã€åºåˆ—é•¿åº¦é™åˆ¶ä»¥åŠæ¨ç†ç›¸å…³çš„åŠŸèƒ½è®¾ç½®ã€‚\n",
    "\n",
    "> æ³¨æ„ï¼šå¦‚æœæƒ³è¦å¯¹ vllm æœåŠ¡è¿›è¡Œæ›´å¤šå®šåˆ¶åŒ–é…ç½®ï¼Œå»ºè®®å‚è€ƒ vllm å®˜æ–¹æ–‡æ¡£ã€‚\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da0650a5",
   "metadata": {},
   "source": [
    "ç„¶åå°±å¯ä»¥åƒä½¿ç”¨ 2.1 äº‘ç«¯å¤§æ¨¡å‹ä¸€æ ·çš„æ–¹å¼æ¥è°ƒç”¨ vllm å¯åŠ¨çš„æ¨¡å‹æœåŠ¡ï¼Œå¦‚ä¸‹ä»£ç æ‰€ç¤ºï¼š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "421ca20f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI(api_key=\"xxx\", \n",
    "                base_url=\"http://127.0.0.1:8000/v1\")\n",
    "response = client.chat.completions.create(\n",
    "    model=\"Qwen3-4B\",\n",
    "    messages=[\n",
    "        {'role': 'user', 'content': \"ä½ å¥½ï¼\"}\n",
    "    ],\n",
    "    max_tokens=1024,\n",
    "    temperature=0.7,\n",
    "    stream=False\n",
    ")\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8540ab6e",
   "metadata": {},
   "source": [
    "## è¯¾åä½œä¸š\n",
    "\n",
    "è¯·åŒå­¦ä»¬å°è¯•ä½¿ç”¨ python ä¸­çš„ requests æ¥è°ƒç”¨æ¨¡å‹æœåŠ¡ã€‚"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
